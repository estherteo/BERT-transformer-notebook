{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estherteo/BERT-transformer-notebook/blob/main/BERT_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKuxU4vGX4hS"
      },
      "source": [
        "# ü§ñ BERT Fine-Tuning for Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wy0v1ObV3_s"
      },
      "source": [
        "In this homework you will revisit the sentiment classification task that we did in Homework 1. However, instead of using Naive Bayes and logistic regression, you will build a neural network classifier by fine-tuning a pretrained language model. This approach to building classifiers is the current state-of-the-art, so learning how to do this should set you up very well for many future NLP problems you may want to tackle.\n",
        "\n",
        "**Note**: As on previous assignments, you are allowed/encouraged to discuss with your classmates, but everything you submit must be your own work and you must understand everything you submit. The use of ChatGPT or other Generative AI tools is not permitted. Please list the names of everyone you worked with, as well as any resources you used.\n",
        "\n",
        "An important goal of this assignment is to teach you how to use online documentation. The world of deep learning libraries and codebases --specifically for NLP-- is rapidly evolving as interest in the field is growing and researchers increasingly opensource their code. As a result, a critical skill of successful NLP researchers and engineers is the ability to learn how to write code on one's own by reading documentation and looking at examples. [PyTorch](https://pytorch.org/docs/stable/index.html) and [Hugging Face](https://huggingface.co/docs) have great documentation. You may use whatever documentation you'd like on this homework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4FgCUY-AaCK"
      },
      "source": [
        "### ‚ùó **Remember to copy and save this notebook before starting** ‚ùó\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXopOZfLYCbR"
      },
      "source": [
        "## Imports and package installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqOAry_cFBTq"
      },
      "source": [
        "These are all of the packages available for use during this assignment, they're all you should need, but you are free to add others. They must first be installed on this runtime, and then imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w-aANJIcYwgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd4b4f0-f54e-4c08-b2e0-bf05263d866b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch datasets tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "MbfkZTYBpCeR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IUgKIvOaX1m_"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import torch, codecs, random\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import datasets\n",
        "from datasets import load_metric\n",
        "from google.colab import output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from transformers import PreTrainedTokenizer\n",
        "from torch.optim import Optimizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# Uncomment if using Drive to upload the dataset\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX2vi2LFer_D"
      },
      "source": [
        "If the following fails, your current Colab runtime doesn't have a GPU. To fix this, simply select 'Runtime' -> 'Change runtime type', then select 'GPU' under 'Hardware accelerator' and then press save."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tJtwvP5Yensh"
      },
      "outputs": [],
      "source": [
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDpb_V8vZFO0"
      },
      "source": [
        "## Part 1 - Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Reading in the data"
      ],
      "metadata": {
        "id": "tfX_8BvjPxkZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16kUKSyqE1C-"
      },
      "source": [
        "You will use the same dataset as in Homework 1. As a reminder, the dataset contains a total of 5,000 tweets labeled with their sentiment. The first 3,000 will be used in training. The next 1,000 will be used as a validation set, and the final 1,000 will be used for testing. The `'text'` column contains a raw Tweet, and the `'sentiment'` column contains the label (`'negative'`, `'neutral'`, or `'positive'`).\n",
        "\n",
        "We need to load the dataset.\n",
        "\n",
        "You can do this by uploading the dataset to this runtime:\n",
        "\n",
        "1. Click on the 'Files' tab of the left sidebar\n",
        "2. Click on the 'Upload' button that should now be exposed, and select the dataset from wherever you downloaded it\n",
        "3. Change the line `read_csv(FILEPATH, ...)` to `read_csv(\"Tweets_5K.csv\", ...)`\n",
        "\n",
        "However, you will have to reupload the file every time your runtime changes. You can also just update the `FOLDER` variable provided to redirect to where you have the data stored in your Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a3OJcfzUZUQY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "FOLDER = \"/content\"\n",
        "FILEPATH = f\"{FOLDER}/Tweets_5K.csv\"\n",
        "\n",
        "# header = 0 - means first row contains the col names\n",
        "dataset = pd.read_csv(FILEPATH, header=0, sep=',')\n",
        "dataset\n",
        "\n",
        "# X should be raw tweet text, y should be integer labels from the sentiment column ('negative': 0, 'neutral': 1, 'positive': 2)\n",
        "# X: List<String>\n",
        "# y: List<int> (can be list-like)\n",
        "X = dataset[\"text\"].to_list()\n",
        "\n",
        "sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "dataset[\"sentiment\"] = dataset[\"sentiment\"].apply(sentiment_mapping.get)\n",
        "# print(dataset.head)\n",
        "\n",
        "y = dataset[\"sentiment\"].to_list()\n",
        "\n",
        "# print(X)\n",
        "# print(y)\n",
        "\n",
        "# # split X and y into a train/validation/test set, use fixed-indices (3000/1000/1000)\n",
        "# trainset, trainlabs = ..., ...\n",
        "# valset, vallabs = ..., ...\n",
        "# testset, testlabs = ..., ...\n",
        "trainset = X[:3000]\n",
        "valset = X[3000:4000]\n",
        "testset = X[4000:]\n",
        "\n",
        "trainlabs = y[:3000]\n",
        "vallabs = y[3000:4000]\n",
        "testlabs = y[4000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preparing the dataset"
      ],
      "metadata": {
        "id": "FA-QT89IQU-r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NVKMPiSZniO"
      },
      "source": [
        "\n",
        "As we saw in class, the models you will be working with require the input to be structured in a very particular way (for example, BERT requires [CLS] and [SEP] tokens). To set up our data and to read it into our models, we will use Pytorch's `Dataset` and `DataLoader`, which you can read about here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html.\n",
        "\n",
        "This is a custom format you can use across projects. Here, we will define a child class to the Pytorch `Dataset` class, such that we can customize it for our specific dataset, and also configure the tokenizer (which we'll initialize in a moment), which will get our data set up in the format required for the models. A custom `Dataset` class requires three functions:  `__init__`, `__len__`, and `__getitem__`.  The `__getitem__` function is what will be used to fetch all training examples by the eventual dataloader. `__getitem__` will tokenize the input tweets in the BERT fashion ([CLS] and [SEP] tokens need to be inserted as discussed in lecture, etc.)\n",
        "\n",
        "This has been written for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x09KaXlaZmvo"
      },
      "outputs": [],
      "source": [
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  A PyTorch Dataset for our tweets that can be iterated through using __getitem__\n",
        "  \"\"\"\n",
        "  def __init__(self, tweets : List[str], sentiments : List[int], tokenizer : PreTrainedTokenizer) -> None:\n",
        "    \"\"\"\n",
        "    Initializes the TweetDataset from a list of tweets, their corresponding sentiments, and a tokenizer.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    tweets : List[str]\n",
        "      A list of tweets, where each tweet is a string\n",
        "    sentiments: List[int]\n",
        "      A list of sentiments represented as integers ('negative': 0, 'neutral': 1, 'positive': 2)\n",
        "    tokenizer : PreTrainedTokenizer\n",
        "      Any PreTrainedTokenizer from HuggingFace can be used to encode the string inputs for a model\n",
        "    \"\"\"\n",
        "    self.tweets = tweets\n",
        "    self.sentiments = sentiments\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = tokenizer.model_max_length\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the number of tweets in the dataset.\n",
        "    \"\"\"\n",
        "    return len(self.tweets)\n",
        "\n",
        "  def __getitem__(self, index : int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Retrieve a preprocessed data item from the dataset at the specified index.\n",
        "    This is called when iterating through a TweetDataset\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    index : int\n",
        "        The index of the data item to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing the preprocessed data for the given index.\n",
        "        The dictionary includes the following keys:\n",
        "        - 'input_ids': Encoded input IDs for the tweet.\n",
        "        - 'attention_mask': Attention mask for the tweet.\n",
        "        - 'labels': Sentiment label as a PyTorch tensor.\n",
        "    \"\"\"\n",
        "    tweet = str(self.tweets[index])\n",
        "    sentiments = self.sentiments[index]\n",
        "\n",
        "    encoded_tweet = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens      = True,\n",
        "      max_length              = self.max_len,\n",
        "      return_token_type_ids   = False,\n",
        "      return_attention_mask   = True,\n",
        "      return_tensors          = \"pt\",\n",
        "      padding                 = \"max_length\",\n",
        "      truncation              = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'input_ids': encoded_tweet['input_ids'][0],\n",
        "      'attention_mask': encoded_tweet['attention_mask'][0],\n",
        "      'labels': torch.tensor(sentiments, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj7nxDf4ahUS"
      },
      "source": [
        "### 3. Tokenizer and dataloader:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXOMe3837Z0t"
      },
      "source": [
        "Now that we have the `TweetDataset` class, you must construct the appropriate instances for our training, validation, and testing sets. Afterwhich, they must be used to construct PyTorch `DataLoader` instances (refer to PyTorch [documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)).\n",
        "\n",
        "At this point, batch sizes will have to be specified. The batch size dictates how many training instances are seen before the model's parameters are updated in gradient descent. For example, with a batch size of 32, the model will process 32 input sentences, before updating weights/parameters. Choosing the batch size may require some experimentation. The larger the batch size, the faster the models will train (because you aren't calculating gradients and updating as often). However, larger batch sizes will also use more VMEM (Video Memory, like RAM, but on a GPU). If you get an error during training or testing about running out of memory, reduce your batch sizes here. I would recommend using a batch size of 32 to start.\n",
        "\n",
        "Finally, we'll use the distilbert-base-uncased tokenizer from Hugging Face to preprocess our tweets. This matches the model we will use down the line, and ensures our input is in the right format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vYg7GtKZbBuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7c61e0-0691-49b4-b5e4-396f3963e14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# use this for the tokenizer argument of the TweetDataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# define the following TweetDataset objects... be careful to split the data as previously specified\n",
        "train_dataset = TweetDataset(trainset, trainlabs, tokenizer)\n",
        "validation_dataset = TweetDataset(valset, vallabs, tokenizer)\n",
        "test_dataset = TweetDataset(testset, testlabs, tokenizer)\n",
        "\n",
        "# now construct DataLoader objects from the TweetDataset objects\n",
        "# remember that the TweetDataset class is a child class of torch.utils.data.Dataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=15, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=15, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Untrained Model (Baseline)"
      ],
      "metadata": {
        "id": "zu_jZWQPU7I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the homework, you will start by working with a completely untrained model and testing how well it does on the sentiment analysis task."
      ],
      "metadata": {
        "id": "5bYiVi_dV2vo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI6K41lqbibh"
      },
      "source": [
        "### 1. Loading untrained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNlsYbbcDGPI"
      },
      "source": [
        "For this homework, you will be working with a model called DistilBERT, which is usually trained for a masked language modeling task. You can read more about it [here](https://huggingface.co/distilbert-base-uncased). It's basically a smaller version of BERT, which was covered in lecture (it's been '[distilled](https://arxiv.org/abs/1910.01108)').\n",
        "\n",
        "Here, you will use the Hugging Face transformers library to load the architecture of the distilbert-base-uncased model. You want to load the UNTRAINED model, meaning you will set up the architecture, but the weights will all be randomly initialized (i.e., this is what the model looks like before any gradient descent is done)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s9K1pZiZb0fd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "# getting the distilbert-base-uncased config...\n",
        "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "# ...from which we can use AutoModelForSequenceClassification to instantiate\n",
        "# an UNTRAINED version of the above model, read Hugging Face documentation\n",
        "# to figure out how to do so!\n",
        "untrained_model = AutoModelForSequenceClassification.from_config(config)\n",
        "# this is untrained, to load the weights instead of .from_config do .from_pretrained()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-9a2VlnJOB"
      },
      "source": [
        "### 2. Write function to evaluate model predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb2OwkVGum5v"
      },
      "source": [
        "We need to define an evaluation function, such that we can measure a model's performance on a dataset. Here, we will be looking at accuracy.\n",
        "\n",
        "For evaluation, there are a couple of steps that must be taken. For each batch in the evaluation set:\n",
        "1. Apply the model on the batch (feedforward pass through the model)\n",
        "2. Obtain the predictions from the model output\n",
        "3. Update the metrics and progress bar\n",
        "\n",
        "Basically, your task is to finish the evaluate function so that it takes a model and test set and outputs what the model's accuracy is on that test set. You need not use the starter code provided, as long as the inputs/outputs match what is given.\n",
        "\n",
        "Hugging Face documentation is your friend - the function signature is also provided.\n",
        "\n",
        "**A note on logits**: DistilBERT's classifier outputs \"logits\". Logits are unnormalized predictions of a model (i.e., this is the input to the softmax function we've discussed in class). In this case, we have three classes: negative, neutral, or positive. One can assign a single output at the end of a model to each of these classes. For the purpose of this homework, we will define the model's prediction to be the class with the maximum logit value (i.e., take the argmax across the outputs). That is, the argmax of the logits is our prediction, which can be compared to the label. This may be useful when tracking metrics. Note: Softmax takes the logits as input and acts as a normalizing function, which outputs a probability distribution over the outputs. But, the argmax of the logits will be the same as the argmax of the softmax output, so no need to use softmax here.\n",
        "\n",
        "**A note on PyTorch devices**\n",
        "\n",
        "In PyTorch, tensor operations can be performed on devices, usually CPUs and GPUs. Tensors can be set to certain devices to optimize operations through parallelization. The operation to set a tensor to a device can be seen [here](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html). Note that the returned tensor is now a copy if the device is changed. **Hint**: Setting a tensor to the correct device is necessary when using them for a model on the GPU!\n",
        "\n",
        "In the code below, the model's parameters are loaded onto the device we are using. This gives us a way to use the GPU to run models by putting all of its parameters and computations on it!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_jxwtMoG_K2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa7127b-f21f-4ea7-acae-8ee7208e02f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# For any parameter named 'device', pass the following variable:\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Loading the model onto the device\n",
        "untrained_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4HrfvEdoABqU"
      },
      "outputs": [],
      "source": [
        "def update_metrics(metrics: List[datasets.Metric], predictions: torch.Tensor, labels: torch.Tensor) -> None:\n",
        "  \"\"\"\n",
        "  Update a list of metrics with new predictions and labels\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  metrics : List[Metric]\n",
        "      List of metrics.\n",
        "  predictions : torch.Tensor\n",
        "      Tensor of predictions of shape (batch_size, ...)\n",
        "  labels : torch.Tensor\n",
        "      Tensor of labels of shape (batch_size, ...)\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  None\n",
        "  \"\"\"\n",
        "  # Nothing TODO here! This updates metrics based on a batch of predictions\n",
        "  # and a batch of labels.\n",
        "  for metric in metrics:\n",
        "    metric.add_batch(predictions=predictions, references=labels)\n",
        "\n",
        "def evaluate(model: torch.nn.Module, test_dataloader: torch.utils.data.DataLoader,\n",
        "             device: torch.device, metric_strs: List[str]) -> Dict[str, float]:\n",
        "  \"\"\"\n",
        "  Evaluate a PyTorch Model\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  model : torch.nn.Module\n",
        "      The model to be evaluated.\n",
        "  test_dataloader : torch.utils.data.DataLoader\n",
        "      DataLoader containing testing examples.\n",
        "  device : torch.device\n",
        "      The device that the evaluation will be performed on.\n",
        "  metric_strs : List[str]\n",
        "      The names of Hugging Face metrics to use.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  Dict[str, float]\n",
        "      Dictionary of metric names mapped to their values.\n",
        "  \"\"\"\n",
        "  # load metrics\n",
        "  metrics = [load_metric(x) for x in metric_strs] # could add more here!\n",
        "  model.eval()\n",
        "\n",
        "  # we like progress bars :)\n",
        "  progress_bar = tqdm(range(len(test_dataloader)))\n",
        "  for sample in test_dataloader:\n",
        "      preds = sample['input_ids']\n",
        "      labs = sample['labels']\n",
        "      attention_mask = sample['attention_mask']\n",
        "\n",
        "      # Move the data to the device (CPU or GPU)\n",
        "      preds = preds.to(device)\n",
        "      labs = labs.to(device)\n",
        "      attention_mask = attention_mask.to(device)\n",
        "\n",
        "      # Feedforward pass\n",
        "      outputs = model(preds, attention_mask)\n",
        "      logits = outputs.logits\n",
        "\n",
        "      # Detach logits to free memory\n",
        "      predictions = logits.detach()\n",
        "      predicted = torch.argmax(logits, dim=1)\n",
        "      update_metrics(metrics, predicted, labs)\n",
        "\n",
        "      progress_bar.update(1)\n",
        "\n",
        "  # HINT: progress_bar.update(1) should be used to show progress after an iteration\n",
        "\n",
        "  # TODO: Fill in the evaluate function by applying the model with the dataloader\n",
        "\n",
        " # HINT: Use .detach() before passing the predictions into update_metrics to prevent memory issues\n",
        "  # and to send the model inputs to the device the model is on\n",
        "  # predictions = outputs.detach()\n",
        "  # Update metrics using update_metrics function\n",
        "  # predictions = outputs.logits.detach()\n",
        "\n",
        "  # compute and return metrics (nothing TODO here)\n",
        "  computed = {}\n",
        "  for m in metrics:\n",
        "    computed = {**computed, **m.compute()}\n",
        "\n",
        "  return computed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evVXkQgQqZ59"
      },
      "source": [
        "### 3. Baseline performance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PII9bwfJUgKM"
      },
      "source": [
        "*Warning: Be aware that since this is a Jupyter Notebook, sometimes it is necessary to re-execute the cell defining the models in order to restart training from scratch. Also, sometimes 'CUDA out of memory' errors are due to past models residing on the GPU, the easiest way to fix this is to restart the Colab runtime (CTRL + M + .), or to run a cell that deletes models (`del <model>`). A batch size of 32 for training should be small enough such that models train without memory issues. To change the batch size, modify the `batch_size` parameter passed when initializing the `DataLoader`.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xataBCpEqfvT"
      },
      "source": [
        "Now that we have an untrained model, our dataset, and an evaluation function, we can evaluate the to find our baseline accuracies! Let's see how well a model that is completely untrained does.\n",
        "\n",
        "The next cell contains some code to make accuracy plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bvp234nxqUrY"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import describe\n",
        "from math import sqrt\n",
        "\n",
        "def make_plots(perfs: List[float], names: List[str]) -> None:\n",
        "  \"\"\"\n",
        "  Create bar plots for performance metrics.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  perfs : List[float]\n",
        "      List of performance values (e.g., accuracies).\n",
        "  names : List[str]\n",
        "      List of names for the corresponding performance metrics.\n",
        "  \"\"\"\n",
        "  # Nothing TODO here\n",
        "  for perf, name in zip(perfs, names):\n",
        "    print(f\"{name} accuracy: {perf}\")\n",
        "  plt.bar(np.arange(len(perfs)), perfs)\n",
        "  plt.xticks(np.arange(len(names)), names)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "AYXUG8OSudaF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHuG9Utbq0Bi"
      },
      "source": [
        "Now, let's execute our evaluation loop on the testing set. In terms of metrics, we'll only keep track of accuracy for now, however you can add others to the metric list (check the Hugging Face datasets documentation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JNhf8-7QsIwe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479,
          "referenced_widgets": [
            "48856a2ce8774687885ea74662a19425",
            "917fee03ce0f4ce9a5bc91079ab3e38e",
            "b6cda8a45d4d4b0fb4d5c71687337d7a",
            "207f2f1d8b3348cc846b93094cbe2a57",
            "21b0c2e260e8455eac092f9342c1eecf",
            "efe4b630e7e5422bb1c9c8991bd7e2f0",
            "2395a907fd9242f6b2611fef2ab264b5",
            "13cbfd1b19634db5b58bc4813e743f73",
            "681399b9987e4de5bc50b1f963b5f61b",
            "ac749a35dcf4459cbd84306ef15352d1",
            "fca2d9339ebe4227a32c3927f3c668be"
          ]
        },
        "outputId": "ca16d32d-d79f-4b66-9d17-a90faaae1d6f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/67 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48856a2ce8774687885ea74662a19425"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 0.319\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "metrics = ['accuracy']\n",
        "\n",
        "# TODO: evaluate on the test dataset\n",
        "baseline_result = evaluate(untrained_model, test_dataloader, device, metrics)['accuracy']\n",
        "\n",
        "# now, plot! Do not modify the following:\n",
        "results = [baseline_result]\n",
        "names = ['Baseline']\n",
        "make_plots(results, names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkBSLJGGcH5n"
      },
      "source": [
        "## Part 3 - Model trained for sentiment analysis from scatch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the homework, you will take your completely untrained model and train it to perform sentiment analysis. That is, you will update the weights so as to get as good performance as possible at predicting sentiment."
      ],
      "metadata": {
        "id": "fXrLVul7ZNBb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2zGkeKvCo2P"
      },
      "source": [
        "### 1. Write a function to train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIRTopzAtU-V"
      },
      "source": [
        "The training loop will follow similar steps to the evaluation loop:\n",
        "For each batch in each epoch (an epoch is one pass through all of the training data), you will:\n",
        "1. Apply the model on the batch inputs (feedforward pass through the model)\n",
        "2. Obtain the predictions, as well as the labels\n",
        "3. Compare the predictions with the labels and calculate the loss\n",
        "4. Backpropagate the loss and obtain gradients for all parameters\n",
        "5. Update the optimizer and learning rate\n",
        "6. Record the desired metrics based on the outputs and labels\n",
        "In addition to this, we'd like to keep track of some metrics during each epoch, and at the end of an epoch we want to run an evaluation on the validation set (you can use the evaluation loop!). Specifically, we want to know the training accuracy (accuracy measured against the training set) and validation accuracy - they can be reported through `print` statements.\n",
        "\n",
        "Reading [PyTorch](https://pytorch.org/docs/stable/index.html) documentation should be very useful here. We'll provide the function signature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mAnyoYIKcaw8"
      },
      "outputs": [],
      "source": [
        "def train(model: torch.nn.Module, optimizer: Optimizer, num_epochs: int,\n",
        "          train_dataloader: DataLoader, validation_dataloader: DataLoader,\n",
        "          lr_scheduler: Any, device: torch.device) -> None:\n",
        "  \"\"\"\n",
        "  Trains a model by performing a forward pass and backpropating on batches to optimize loss.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  model : torch.nn.Module\n",
        "      The model to be trained.\n",
        "  optimizer : torch.optim.Optimizer\n",
        "      The training optimizer.\n",
        "  num_epochs : int\n",
        "      Number of epochs to train for.\n",
        "  train_dataloader : DataLoader\n",
        "      DataLoader containing training examples.\n",
        "  validation_dataloader : DataLoader\n",
        "      DataLoader containing validation examples.\n",
        "  lr_scheduler : Any\n",
        "      Learning rate scheduler.\n",
        "  device : torch.device\n",
        "      The device that the training will be performed on.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  None\n",
        "  \"\"\"\n",
        "\n",
        "  # Define your loss function; Use CrossEntropyLoss\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # added these losss\n",
        "  running_loss = 0.\n",
        "  last_loss = 0.\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # put the model in training mode (important that this is done each epoch,\n",
        "    # since we put the model into eval mode during validation)\n",
        "    model.train()\n",
        "\n",
        "    # load metrics\n",
        "    metrics = [load_metric(x) for x in [\"accuracy\"]] # could add more here!\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} training:\")\n",
        "    progress_bar = tqdm(range(len(train_dataloader)))\n",
        "\n",
        "    # TODO: Fill in the rest of the train function by applying the model with the dataloader\n",
        "    # HINT: Remember to use .detach() before passing model predictions to update_metrics function,\n",
        "\n",
        "    for index, data in enumerate(train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, attention_mask, labels = data['input_ids'], data['attention_mask'], data['labels']\n",
        "\n",
        "        # send the model inputs .to() the device that the model is on\n",
        "        preds = inputs.to(device)\n",
        "        labs = labels.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # Feedforward pass\n",
        "        outputs = model(preds, attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # # Detach logits to free memory\n",
        "        # predictions = logits.detach()\n",
        "\n",
        "        # obtain predictions\n",
        "        predicted = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Zero your gradients for every batch! - as pytorch accumulates the gradients from each batch onto the existing grads\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the loss and its gradients - try replacing logits with predictions\n",
        "        loss = loss_function(logits, labs)\n",
        "        print(logits.dtype)\n",
        "        print(labs.dtype)\n",
        "        print(logits.size())\n",
        "        print(labs.size())\n",
        "\n",
        "        # backpropagate the loss and computes the grads\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights based on the grads\n",
        "        optimizer.step()\n",
        "\n",
        "        # Detach logits to free memory\n",
        "        predictions = logits.detach()\n",
        "\n",
        "        update_metrics(metrics, predicted, labs)\n",
        "\n",
        "        # Gather data and report\n",
        "        # running_loss accumulates the loss value over batches within the epoch\n",
        "        running_loss += loss.item()\n",
        "        if index % 1000 == 999:   # every 1000th batch\n",
        "            last_loss = running_loss / 1000   # calc avg loss per batch\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss\n",
        "\n",
        "    # print the epoch's average metrics\n",
        "    print(f\"Epoch {epoch+1} average training metrics: accuracy={metrics[0].compute()['accuracy']}\")\n",
        "\n",
        "    # normally, validation would be more useful when training for many epochs\n",
        "    print(\"Running validation:\")\n",
        "    # TODO: evaluate model on validation dataset\n",
        "    val_metrics = evaluate(untrained_model, validation_dataloader, device, metrics)\n",
        "    print(f\"Epoch {epoch+1} validation: accuracy={val_metrics['accuracy']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6SIbywfwSWQ"
      },
      "source": [
        "### 2. Training the untrained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAVKwkU7wb5B"
      },
      "source": [
        "Great - now that we've defined our training function, we can use it to try to train the untrained model from scratch! Train the model for **2 epochs**!\n",
        "\n",
        "**A note on warmup steps, optimizers**: An optimizer is a particular algorithm used to minimize the loss function. Optimizers like AdamW try to adapt the learning rate for individual parameters by collecting statistics about them. Warmup impacts some optimizers. Warmup steps allow the statistics calculated by some optimizers to converge before the optimizer starts to update parameters impactfully. During the warmup, the optimizer is updating parameters, but just by a much smaller magnitude than usual. Read more about optimizers [here](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/#:~:text=While%20training%20the%20deep%20learning,loss%20and%20improve%20the%20accuracy.), and warmup [here](https://d2l.ai/chapter_optimization/lr-scheduler.html#warmup). The warmup is handled by the learning rate scheduler behind the scenes.\n",
        "\n",
        "First, we need to set up our optimizer, learning rate scheduler, and determine the number of epochs to train for. These are all parameters to the `train` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fu2xF7SnwwWi"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "# TODO: use the AdamW optimizer. Use torch.optim.AdamW.\n",
        "# instantiate it on the untrained model parameters with a learning rate of 5e-5\n",
        "optimizer = torch.optim.AdamW(untrained_model.parameters(), lr=5e-5)\n",
        "\n",
        "# now, we set up the learning rate scheduler\n",
        "lr_scheduler = get_scheduler(\n",
        "  \"linear\",\n",
        "  optimizer=optimizer,\n",
        "  num_warmup_steps=50,\n",
        "  num_training_steps=len(train_dataloader) * num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jktOLw9xJxS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "50098c7197ac42168632c6faa1f7dfa9",
            "cdb47c8165924fa2a84292405d760d41",
            "d8b7593ebd6c4f42a5fd4a5b0fc9bded",
            "51e1376b34d74945af79a8df8a56c242",
            "f9595666e40a461981b9db55e668e7f1",
            "b8f31b4563a248038568d599008fb1ea",
            "00a163c03f3d4c4bb5a24c1afd378166",
            "928a67d23ed34f77941f90c00cfec8f9",
            "d98edcead08a4a7997d13eae39964cd2",
            "a34edfd3a163433f93af08bda537fd93",
            "42908e16d6bb41988bb9f757ea0225ec"
          ]
        },
        "outputId": "ab6aaa86-7ed2-47fc-d1d4-a62f5eb30a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 training:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50098c7197ac42168632c6faa1f7dfa9"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.Size([15, 3])\n",
            "torch.Size([15])\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "untrained_model.to(device)\n",
        "\n",
        "# TODO: now call your training loop!\n",
        "train(untrained_model, optimizer, num_epochs, train_dataloader, validation_dataloader, lr_scheduler, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgAS0msxxgmd"
      },
      "source": [
        "Hypothetically, the untrained model is now slightly trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkwCN4wrxn1p"
      },
      "source": [
        "### 3. Evaluate the trained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8QstKRRxtpV"
      },
      "source": [
        "Call the evaluation loop on the model we just trained! View the bar graph to compare it to the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG8wV4c8yJb_"
      },
      "outputs": [],
      "source": [
        "metrics = ['accuracy']\n",
        "\n",
        "# TODO: Evaluate on test dataset!\n",
        "trained_result = evaluate(untrained_model, test_dataloader, device, metrics)['accuracy']\n",
        "\n",
        "# now, plot! Do not modify the following:\n",
        "results = baseline_result, trained_result\n",
        "names = ['Baseline', 'Manually Trained']\n",
        "make_plots(results, names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVY1H7SAyyxP"
      },
      "source": [
        "As you can see, this model achieves an accuracy of around 55% after the first two epochs. Training it for further epochs (which you don't need to do) renders minimal improvement. To achieve better performance when training DistilBERT from scratch for this purpose, we'd need a lot more data and a lot more time (and/or compute). Fortunately, there is a better way!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqqmGK5ryeRr"
      },
      "source": [
        "## Part 4 - Finetuning a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEoT4c2Cymoz"
      },
      "source": [
        "Since manually training the model for sentiment analysis didn't work very well, let's fine-tune a pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lWnuLCFy8sJ"
      },
      "source": [
        "### 1. Load pre-trained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayo06S74lx4"
      },
      "source": [
        "This time, we'll make use of a pre-trained version of DistilBERT - everything's the same as before, but the model has actually already been trained for a masked language modeling task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbuZY_Xzy8Gc"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "# TODO: load the distilbert-base-uncased pre-trained model, use DistilBertForSequenceClassification\n",
        "# from Hugging Face (transformers), read their documentation to do this.\n",
        "# WARNING: This dataset has three classes! The default for this model is two!\n",
        "pretrained_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Ignore the warnings printed below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUnaNxCSz0Go"
      },
      "source": [
        "Let's give it a test run...\n",
        "To motivate, the pre-trained model isn't particularly good at this specific task out-of-the box, to see this, evaluate it on the test set just the way it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlMMfY620Ud1"
      },
      "outputs": [],
      "source": [
        "# We have to move it to the device before doing anything with it!\n",
        "pretrained_model.to(device)\n",
        "\n",
        "# TODO: call evaluate on the pre-trained model!\n",
        "pretrained_result = evaluate(pretrained_model, test_dataloader, device, metrics)['accuracy']\n",
        "\n",
        "# now, plot! Do not modify the following:\n",
        "results = [baseline_result, trained_result, pretrained_result]\n",
        "names = ['Baseline', 'Manually trained', 'Pre-trained']\n",
        "make_plots(results, names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsBRmaPq0yx-"
      },
      "source": [
        "As we can see, without any fine-tuning, the pre-trained model performs rather badly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSdPDbOL1qBV"
      },
      "source": [
        "### 2. Fine-tuning pre-trained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cC-gLLX0Iww"
      },
      "source": [
        "Rather than train the model from scratch, we'll now fine-tune the pre-trained model! That is, we will take the model that was trained to predict masked words,\n",
        "and now train it on our sentiment analysis task.\n",
        "We'll fine-tune by simply running the `train` function on our pre-trained model with the training data! Fine-tune it for **2 epochs**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTW6T8bH0yga"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "\n",
        "# TODO: use the same kind of optimizer as before, but with the pretrained model's parameters\n",
        "optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=5e-5)\n",
        "\n",
        "# this hasn't changed\n",
        "lr_scheduler = get_scheduler(\n",
        "  \"linear\",\n",
        "  optimizer=optimizer,\n",
        "  num_warmup_steps=50,\n",
        "  num_training_steps=len(train_dataloader) * num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZEmlHFM1Wt7"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "pretrained_model.to(device)\n",
        "\n",
        "# TODO: train!\n",
        "train(pretrained_model, optimizer, num_epochs, train_dataloader, validation_dataloader, lr_scheduler, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4rOFE7D1bpu"
      },
      "source": [
        "You should've seen a sneak-peak of the model's performance based on the validation accuracies!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHLUYPvY1kjC"
      },
      "source": [
        "### 3. Evaluating the fine-tuned model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d15yLO5L1yf4"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate the fine-tuned model on the test dataset\n",
        "finetuned_result = evaluate(pretrained_model, test_dataloader, device, metrics)['accuracy']\n",
        "\n",
        "# now, plot! Do not modify the following:\n",
        "results = [baseline_result, trained_result, pretrained_result, finetuned_result]\n",
        "names = ['Baseline', 'Manually trained', 'Pre-trained', 'Fine-tuned']\n",
        "make_plots(results, names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MgaiZQX3Uyo"
      },
      "source": [
        "Wow! With just two epochs of training, and only 3k training examples, we can achieve ~75% accuracy (this should be attainable)! As we can see, the pre-trained model has a very noticeable advantage over the completely untrained one. Training for an additional epoch renders a slight improvement, afterwhich the fine-tuning begins to overfit on our training set, which can be judged by comparing the average epoch training accuracies and validation accuracies.\n",
        "\n",
        "Additionally, remember that this was the dataset used in assignment 1, where you had to do significant pre-processing to get the accuracy to just ~65%! With ease (and less training data, since here we had a 60/20/20 split), DistilBERT has an even better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKgscWfnz_PQ"
      },
      "source": [
        "## Part 5 - Conceptual Questions\n",
        "\n",
        "1. Take a look at Dataset and the BERT tokenizer (as well as their documentation). Print the output of `__getitem__` for one tweet. What does `__getitem__` return? Describe what the three components are, what their contents are (be as concrete as possible), and how they correspond to what we've seen in class. Include in your answer: describe what the numbers you see are referring to (broadly), what the zeros mean, etc. In other words, describe as concretely as possible what the input to DistilBERT needs to be.\n",
        "\n",
        "2. For each version of the model that you evaluated, describe what that model was trained to do (if anything). Report the accuracy observed for each. Do these relative accuracies make sense to you? Briefly explain why each model performs the way it does. If a model performs well, why does it perform well? If a model performs poorly, why does it perform poorly?\n",
        "\n",
        "3. In this assignment we've covered several methods of getting a language model to do a task (in this case, sentiment analysis): (i) Using a pretrained model off-the-shelf; (ii) Manually training a model from scratch; and (iii) Fine-tuning a pretrained model. People have to balance various factors when considering which method to adopt in deploying language models. List at least two factors that a person may wish to consider and their implications for which method(s) would be best.\n",
        "\n",
        "How long (roughly) did it take you to complete this assignment?\n",
        "\n",
        "Note: From here, you can make relatively easy switches. For example, you can easily sub in BERT instead of DistilBERT, or swap DistilBertForTokenClassification or DistilBertForMaskedLM or DistilBertForQuestionAnswering to consider a different task. All of NLP is at your fingertips :)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The input to DistilBERT needs to be [CLS] X [SEP], where CLS is the classifier token that is used during sequence classification and SEP is the separator token that is used when building a sequence from multiple sequences."
      ],
      "metadata": {
        "id": "STtrGQ06mvSg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGT9K4v66Dv6"
      },
      "source": [
        "## Hand-in:\n",
        "To hand-in, first delete any debugging print statements, this helps me read your code.\n",
        "\n",
        "Then, go to Runtime->Restart and run all to run the entire notebook with your completed code.\n",
        "\n",
        "Finally, submit TWO files:\n",
        "(1) download the notebook as a .ipynb, and\n",
        "(2) go to File > Print > Save as PDF.\n",
        "\n",
        "Upload the **.ipynb AND .pdf** to Blackboard.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "cd309e6aeab0bd9d83c4bd3bfed081617b53e9cd2894363ef80f097c73d52faa"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48856a2ce8774687885ea74662a19425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_917fee03ce0f4ce9a5bc91079ab3e38e",
              "IPY_MODEL_b6cda8a45d4d4b0fb4d5c71687337d7a",
              "IPY_MODEL_207f2f1d8b3348cc846b93094cbe2a57"
            ],
            "layout": "IPY_MODEL_21b0c2e260e8455eac092f9342c1eecf"
          }
        },
        "917fee03ce0f4ce9a5bc91079ab3e38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efe4b630e7e5422bb1c9c8991bd7e2f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2395a907fd9242f6b2611fef2ab264b5",
            "value": "100%"
          }
        },
        "b6cda8a45d4d4b0fb4d5c71687337d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13cbfd1b19634db5b58bc4813e743f73",
            "max": 67,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_681399b9987e4de5bc50b1f963b5f61b",
            "value": 67
          }
        },
        "207f2f1d8b3348cc846b93094cbe2a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac749a35dcf4459cbd84306ef15352d1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fca2d9339ebe4227a32c3927f3c668be",
            "value": "‚Äá67/67‚Äá[00:24&lt;00:00,‚Äá‚Äá2.98it/s]"
          }
        },
        "21b0c2e260e8455eac092f9342c1eecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe4b630e7e5422bb1c9c8991bd7e2f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2395a907fd9242f6b2611fef2ab264b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13cbfd1b19634db5b58bc4813e743f73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "681399b9987e4de5bc50b1f963b5f61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac749a35dcf4459cbd84306ef15352d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fca2d9339ebe4227a32c3927f3c668be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50098c7197ac42168632c6faa1f7dfa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdb47c8165924fa2a84292405d760d41",
              "IPY_MODEL_d8b7593ebd6c4f42a5fd4a5b0fc9bded",
              "IPY_MODEL_51e1376b34d74945af79a8df8a56c242"
            ],
            "layout": "IPY_MODEL_f9595666e40a461981b9db55e668e7f1"
          }
        },
        "cdb47c8165924fa2a84292405d760d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8f31b4563a248038568d599008fb1ea",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_00a163c03f3d4c4bb5a24c1afd378166",
            "value": "‚Äá‚Äá0%"
          }
        },
        "d8b7593ebd6c4f42a5fd4a5b0fc9bded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928a67d23ed34f77941f90c00cfec8f9",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d98edcead08a4a7997d13eae39964cd2",
            "value": 0
          }
        },
        "51e1376b34d74945af79a8df8a56c242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34edfd3a163433f93af08bda537fd93",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_42908e16d6bb41988bb9f757ea0225ec",
            "value": "‚Äá0/200‚Äá[00:00&lt;?,‚Äá?it/s]"
          }
        },
        "f9595666e40a461981b9db55e668e7f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8f31b4563a248038568d599008fb1ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a163c03f3d4c4bb5a24c1afd378166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "928a67d23ed34f77941f90c00cfec8f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d98edcead08a4a7997d13eae39964cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a34edfd3a163433f93af08bda537fd93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42908e16d6bb41988bb9f757ea0225ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}